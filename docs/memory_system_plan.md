# Vexa Persistent Memory System - Implementation Plan

## Executive Summary

This document outlines the architecture and implementation plan for a three-tiered persistent memory system that mimics human memory patterns. The goal is to eliminate discrete "chat sessions" in favor of continuous conversation with progressively fuzzier historical memory.

---

## Current State Analysis

### Existing Components

**Orchestrator** (`lib/orchestrator.py`)
- Already implements basic memory archival when context window exceeds limits
- Has stub `MemoryBackend` with in-memory list storage
- Archives the Nth conversation entry (default: index 1) when `max_conversation_length` is exceeded
- Recalls last 3 entries and injects them into system prompt
- **Limitation**: Non-persistent, no semantic search, simple FIFO archival

**VexaApp** (`vexa.py`)
- Manages conversation array with system prompt at index 0
- Implements context window truncation (keeps first + last N-1 messages)
- Current `max_conversation_length` default: 100 messages
- **Limitation**: Hard truncation loses middle conversation context permanently

**Test Files**
- `test/chromadb_cpu.py` demonstrates ChromaDB with sentence-transformers embedding
- Uses `all-MiniLM-L6-v2` model (384-dimensional embeddings, fast CPU inference)

---

## Memory Architecture Design

### Three-Tier Memory System

```
┌─────────────────────────────────────────────────────────────┐
│                    L1: Context Window                       │
│  ┌────────────────────────────────────────────────────┐    │
│  │ [System Prompt] + Last N messages (raw)            │    │
│  │ Size: ~20-100 messages (configurable)              │    │
│  │ Retention: Active conversation only                │    │
│  │ Fidelity: 100% (exact messages)                    │    │
│  └────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
                            ↓ Archive (overflow)
┌─────────────────────────────────────────────────────────────┐
│               L2: Short-Term Memory (ChromaDB)              │
│  ┌────────────────────────────────────────────────────┐    │
│  │ Summarized conversation chunks (2-5 message pairs)  │    │
│  │ Size: Last ~500-1000 turns                         │    │
│  │ Retention: Days to weeks                           │    │
│  │ Fidelity: 70-80% (semantic summaries)              │    │
│  │ Access: Vector similarity search                   │    │
│  └────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
                            ↓ Compress (scheduled)
┌─────────────────────────────────────────────────────────────┐
│               L3: Long-Term Memory (SQLite)                 │
│  ┌────────────────────────────────────────────────────┐    │
│  │ High-level thematic summaries + key facts          │    │
│  │ Size: Unlimited (compressed)                       │    │
│  │ Retention: Permanent                               │    │
│  │ Fidelity: 30-40% (distilled knowledge)             │    │
│  │ Access: Keyword/topic tags + periodic injection    │    │
│  └────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
```

---

## Database Selection & Rationale

### L2: Short-Term Memory - ChromaDB

**Why ChromaDB?**
- ✅ Native Python integration (already tested)
- ✅ Built-in embedding function support
- ✅ Persistent storage with simple configuration
- ✅ Fast vector similarity search (HNSW indexing)
- ✅ Lightweight (no separate server required)
- ✅ CPU-friendly with sentence-transformers
- ✅ Supports metadata filtering (timestamp, importance, topic)

**Schema Design:**
```python
{
    "id": "uuid",
    "document": "summarized_text",  # The searchable summary
    "embedding": [float],  # Auto-generated by ChromaDB
    "metadata": {
        "timestamp": float,  # Unix timestamp
        "conversation_range": "msg_45-50",  # Original message IDs
        "importance": float,  # 0.0-1.0 calculated score
        "topic": str,  # Extracted topic/theme
        "message_count": int,  # Number of messages summarized
        "user_sentiment": str,  # positive/neutral/negative
    }
}
```

**Persistence Path:** `~/.vexa/memory/short_term/`

### L3: Long-Term Memory - SQLite

**Why SQLite?**
- ✅ Zero-configuration, serverless
- ✅ Excellent for structured, relational data
- ✅ Built into Python standard library
- ✅ Efficient full-text search (FTS5 extension)
- ✅ ACID compliance for data integrity
- ✅ Perfect for time-series queries (date ranges)

**Schema Design:**
```sql
CREATE TABLE memory_archives (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    time_period_start TIMESTAMP,
    time_period_end TIMESTAMP,
    summary TEXT NOT NULL,
    key_topics TEXT,  -- JSON array
    key_facts TEXT,   -- JSON array
    emotional_tone TEXT,
    message_count INTEGER,
    source_ids TEXT,  -- JSON array of ChromaDB IDs
    importance_score REAL
);

CREATE INDEX idx_timestamp ON memory_archives(created_at);
CREATE INDEX idx_period ON memory_archives(time_period_start, time_period_end);

-- Full-text search capability
CREATE VIRTUAL TABLE memory_search USING fts5(
    summary, key_topics, key_facts,
    content=memory_archives
);
```

**Persistence Path:** `~/.vexa/memory/long_term/memory.db`

---

## Implementation Phases

### Phase 1: ChromaDB Integration (Short-Term Memory)

**Files to Create/Modify:**

1. **`lib/memory/short_term_memory.py`** (NEW)
   ```python
   class ShortTermMemory:
       - __init__(persist_directory)
       - add_memory_chunk(messages, summary)
       - query_relevant_memories(query_text, top_k, time_filter)
       - get_recent_memories(n)
       - calculate_importance(messages)
       - cleanup_old_memories(days_threshold)
   ```

2. **`lib/memory/summarizer.py`** (NEW)
   ```python
   class ConversationSummarizer:
       - summarize_messages(messages) -> summary
       - extract_topics(messages) -> [topics]
       - analyze_sentiment(messages) -> sentiment_score
   ```

3. **Modify `lib/orchestrator.py`:**
   - Replace stub `MemoryBackend` with `ShortTermMemory`
   - Implement actual summarization before archival
   - Add memory recall with semantic search
   - Inject top-K relevant memories into system prompt

**Configuration (`config/settings.yaml`):**
```yaml
memory:
  short_term:
    enabled: true
    persist_directory: "~/.vexa/memory/short_term"
    chunk_size: 4  # Messages per summary
    max_entries: 1000  # Auto-prune beyond this
    retention_days: 30
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    recall_top_k: 5
```

### Phase 2: Long-Term Memory (Future)

**Files to Create:**

1. **`lib/memory/long_term_memory.py`**
   ```python
   class LongTermMemory:
       - __init__(db_path)
       - create_archive(summary, metadata)
       - query_by_timerange(start, end)
       - query_by_topic(topic)
       - full_text_search(query)
       - get_periodic_context() -> weekly/monthly summaries
   ```

2. **`lib/memory/compression_scheduler.py`**
   ```python
   class MemoryCompressor:
       - compress_short_to_long(days_ago=7)
       - generate_thematic_summary(memory_chunks)
       - extract_key_facts(memory_chunks)
   ```

**Scheduled Task:**
- Daily cron/background task to compress 7+ day old short-term memories
- Keeps ChromaDB lightweight, moves historical data to SQLite

### Phase 3: Memory Lifecycle Manager

**File: `lib/memory/memory_manager.py`**
```python
class MemoryManager:
    """Unified interface for all memory tiers"""
    - __init__(app)
    - archive_from_context_window()
    - recall_relevant_context(query) -> merged memories
    - periodic_compression()
    - export_memory_snapshot()
```

---

## Memory Flow Diagram

```
User Input
    ↓
┌───────────────────────────────────┐
│   Orchestrator.process_prompt()   │
└───────────────────────────────────┘
    ↓
    ├─→ Check context window size
    │       ↓ (if > max_len)
    │   Archive oldest 4 messages
    │       ↓
    │   Summarize via LLM
    │       ↓
    │   Store in ChromaDB (L2)
    │
    ├─→ Query ChromaDB for relevant memories
    │       ↓
    │   Vector search on user input
    │       ↓
    │   Retrieve top-K similar chunks
    │
    └─→ Compose augmented system prompt
            ↓
        [Base System Prompt]
        + [Recent Context Window]
        + [Recalled Short-Term Memories]
        + [Periodic Long-Term Themes] (Phase 2)
            ↓
        Send to LLM
```

---

## Advanced Features & Improvements

### 1. Importance Scoring Algorithm

```python
def calculate_importance(messages: list) -> float:
    """
    Multi-factor importance calculation:
    - Message length (longer = potentially more substantial)
    - Emotional intensity (detected via sentiment analysis)
    - Information density (entities, dates, commitments)
    - User engagement signals (follow-up questions, clarifications)
    """
    score = 0.5  # baseline
    
    # Length factor (diminishing returns)
    avg_length = sum(len(m['content']) for m in messages) / len(messages)
    score += min(avg_length / 1000, 0.2)
    
    # Sentiment extremity (very positive/negative = memorable)
    sentiment_intensity = analyze_sentiment_intensity(messages)
    score += sentiment_intensity * 0.2
    
    # Entity density (names, dates, places)
    entity_count = count_entities(messages)
    score += min(entity_count * 0.05, 0.15)
    
    return min(score, 1.0)
```

### 2. Time-Decay Weighting

Recent memories should be weighted higher in retrieval:

```python
def apply_time_decay(results: list, half_life_days: float = 7.0) -> list:
    """Apply exponential decay to memory relevance scores"""
    now = time.time()
    for result in results:
        age_days = (now - result['timestamp']) / 86400
        decay_factor = 0.5 ** (age_days / half_life_days)
        result['score'] *= decay_factor
    return sorted(results, key=lambda x: x['score'], reverse=True)
```

### 3. Topic Clustering

Group memories by semantic topics for better organization:

```python
# Store topic embeddings separately
# Periodically run k-means clustering on ChromaDB entries
# Tag clusters: "technical discussions", "personal stories", "debugging sessions"
```

### 4. Conversational Continuity Markers

```python
# Add metadata to track:
- conversation_id: UUID for each "natural break" in dialogue
- continuation_score: How smoothly this follows previous context
- reference_count: How many times past memories are recalled
```

### 5. Memory Pruning Strategy

**ChromaDB (L2):**
- Keep last 30 days unconditionally
- For 30-90 days: Keep only importance > 0.6
- Archive 90+ days to SQLite, delete from ChromaDB

**SQLite (L3):**
- Never delete, only compress
- Monthly summaries generated from daily/weekly entries

### 6. Privacy & Export

```python
# User commands:
/memory-export   # Export all memories as JSON
/memory-clear    # Wipe all memories (confirm prompt)
/memory-stats    # Show storage usage, entry counts
/memory-search <query>  # Manual search interface
```

---

## Dependencies Update

**Add to `requirements.txt`:**
```
chromadb>=0.4.0
sentence-transformers>=2.2.0
torch>=2.0.0  # For sentence-transformers
numpy>=1.24.0
```

**Optional for Phase 2:**
```
schedule>=1.2.0  # For background compression tasks
```

---

## Configuration Schema

**Extended `config/settings.yaml`:**
```yaml
memory:
  enabled: true
  
  short_term:
    backend: "chromadb"
    persist_directory: "~/.vexa/memory/short_term"
    chunk_size: 4  # Messages summarized together
    max_entries: 1000
    retention_days: 30
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    recall_top_k: 5
    importance_threshold: 0.3  # Don't store trivial exchanges
    
  long_term:
    enabled: false  # Phase 2
    backend: "sqlite"
    db_path: "~/.vexa/memory/long_term/memory.db"
    compression_schedule: "daily"
    compression_age_days: 7
    
  context_window:
    max_messages: 50  # L1 cache size
    overflow_behavior: "archive"  # or "truncate"
```

---

## Testing Strategy

### Unit Tests

1. **`test/memory/test_short_term_memory.py`**
   - Test ChromaDB initialization
   - Test memory addition and retrieval
   - Test time-based filtering
   - Test importance scoring

2. **`test/memory/test_summarizer.py`**
   - Test summarization quality
   - Test topic extraction
   - Test sentiment analysis

3. **`test/memory/test_orchestrator.py`**
   - Test archival triggers
   - Test memory recall injection
   - Test context window management

### Integration Tests

1. **Conversation Continuity Test:**
   - Simulate 200-message conversation
   - Verify context window stays at max_length
   - Verify old messages are retrievable via semantic search

2. **Long-Running Session Test:**
   - Simulate conversation over multiple days
   - Verify memories persist across app restarts
   - Verify time-decay affects recall order

---

## Performance Considerations

### ChromaDB Optimization

- **Embedding Model Choice:** `all-MiniLM-L6-v2` (384-dim) balances speed/quality
- **Index Type:** HNSW (Hierarchical Navigable Small World) for fast ANN search
- **Batch Operations:** Add memories in batches during archival
- **Collection Sharding:** Consider separate collections per topic cluster (Phase 2)

### SQLite Optimization

- **FTS5 Index:** Enables fast full-text search without scanning
- **Write-Ahead Logging (WAL):** Better concurrency for background compression
- **Periodic VACUUM:** Reclaim space from deleted entries

### Memory Footprint

- **ChromaDB:** ~500KB per 1000 entries (with 384-dim embeddings)
- **SQLite:** ~100KB per 1000 compressed summaries
- **Total for 10K conversation turns:** ~5-10MB (very reasonable)

---

## Risks & Mitigations

### Risk 1: Summarization Quality

**Problem:** LLM-generated summaries may lose critical details

**Mitigation:**
- Store both original message IDs and summaries
- Implement "detail recovery" mode that fetches originals if needed
- A/B test different summarization prompts
- Allow user to manually tag "important" conversations

### Risk 2: Embedding Model Drift

**Problem:** Changing embedding models invalidates existing vectors

**Mitigation:**
- Version the embedding model in metadata
- Implement migration script to re-embed on model change
- Use stable, well-supported models (sentence-transformers)

### Risk 3: Privacy Concerns

**Problem:** Persistent memory stores potentially sensitive data

**Mitigation:**
- Store all data locally (no cloud sync by default)
- Implement `/memory-clear` command
- Add option to disable memory system entirely
- Document data storage locations clearly

### Risk 4: Context Pollution

**Problem:** Irrelevant recalled memories confuse the model

**Mitigation:**
- Tune `recall_top_k` conservatively (start with 3-5)
- Implement relevance threshold (drop results below score X)
- Add temporal filtering (prefer recent memories)
- Monitor conversation quality and adjust parameters

---

## Migration Path from Current System

### Step 1: Backward Compatibility

- Keep existing `max_conversation_length` behavior as fallback
- Add feature flag: `memory.enabled: false` defaults to old behavior

### Step 2: Gradual Rollout

1. Deploy Phase 1 with memory system disabled
2. Enable for testing with small `retention_days` value
3. Gather user feedback on recall quality
4. Tune parameters based on real usage
5. Enable by default in stable release

### Step 3: Data Migration

- No migration needed (fresh start with new memory system)
- Optionally: Parse old conversation logs to seed initial memories

---

## Future Enhancements

### Conversational Memory Types

1. **Episodic Memory:** Specific events ("You told me about your trip to Japan")
2. **Semantic Memory:** General facts ("You prefer Python over JavaScript")
3. **Procedural Memory:** Learned patterns ("You usually ask for examples after explanations")

### Cross-Session Context

- Detect conversation topic shifts
- Auto-generate session boundaries
- Implement "conversation threads" that can be revisited

### Multi-Modal Memory

- Store references to images/files discussed
- Integrate with file system events (remember when files were edited)

### Collaborative Memory

- Multi-user support (separate memory spaces)
- Shared memory pools for team contexts

---

## Success Metrics

1. **Context Coherence:** Model correctly references events from 50+ messages ago
2. **Storage Efficiency:** 10K messages stored in < 10MB
3. **Query Performance:** Semantic search < 100ms for top-5 results
4. **User Satisfaction:** Subjective "AI remembers me" feeling
5. **Conversation Length:** Users naturally have longer, multi-day conversations

---

## Timeline Estimate

- **Phase 1 (ChromaDB):** 2-3 weeks
  - Week 1: Core implementation, basic summarization
  - Week 2: Integration with orchestrator, testing
  - Week 3: Parameter tuning, documentation
  
- **Phase 2 (SQLite):** 1-2 weeks
  - Week 1: Schema design, compression logic
  - Week 2: Background scheduler, testing

- **Phase 3 (Unified Manager):** 1 week
  - Abstraction layer, export/import tools

**Total:** ~4-6 weeks for full implementation

---

## Conclusion

This memory system design balances:
- **Performance:** Lightweight, local, fast vector search
- **Quality:** Multi-tiered fidelity preserves important details
- **Scalability:** Automatic pruning and compression
- **Privacy:** All data stored locally
- **Extensibility:** Clear migration path to advanced features

The architecture mimics human memory (L1/L2/L3 cache analogy) while remaining simple enough to implement incrementally. Starting with ChromaDB for Phase 1 provides immediate value (persistent short-term memory) without requiring the full long-term storage infrastructure.
